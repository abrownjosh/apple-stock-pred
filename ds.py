# -*- coding: utf-8 -*-
"""DS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hCxGTTJjT5kl8dAhvL1y4iUT1DtQCZi_
"""

import pandas as pd
Meta_DS = pd.read_excel('DS_all_data.xlsx',
                        index_col = None,
                        sheet_name = "Meta")
Apple_DS = pd.read_excel('DS_all_data.xlsx',
                         index_col = None,
                         sheet_name = "Apple")
Ebay_DS = pd.read_excel('DS_all_data.xlsx',
                        index_col = None,
                        sheet_name = "Ebay")
Google_DS = pd.read_excel('DS_all_data.xlsx',
                          index_col = None,
                          sheet_name = "Google")

Apple_DS = Apple_DS.reset_index()
date_vals = Apple_DS.Date.values
Apple_DS['Date'] = date_vals[::-1]

import seaborn as sns
import matplotlib.pyplot as plt

#lots of unnecessary warnings about indexing.
import warnings
warnings.filterwarnings('ignore')

# intial plot of apple data
fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15, 5))
fig.tight_layout(pad=3.0)
sns.lineplot(data = Apple_DS,
             x=Apple_DS.index,
             y='Open', ax=ax1).set(xlabel = "Day", title = "Apple Open")
sns.lineplot(data=Apple_DS,
             x=Apple_DS.index,
             y='High', ax=ax2, color = 'purple').set(xlabel = "Day",
                                                     title = "Apple High")
sns.lineplot(data=Apple_DS,
             x=Apple_DS.index,
             y='Low', ax=ax3, color = 'red').set(xlabel = "Day",
                                                 title =  "Apple Low")
plt.show()

Apple_DS['Date'] = pd.to_datetime(Apple_DS['Date'], format = '%d/%m/%y')

#### beginning a time series analysis####


#starting by checking stationary, a measure of how constant series data is.
#stationary datasets do not have seasonal patterns or trends.

#testing for stationary using Dickey Fuller test. Generates a p value and CVs
# null hypothesis si that there is no stationary.

#because we have a year of data, going to calculate a 12 month rolling mu &sigma
rolling_mean = Apple_DS['Open'].rolling(24).mean()
rolling_std = Apple_DS['Open'].rolling(24).std()

#starting with just the opening price.

#plot the open data, the rolling mean,
fig, ax = plt.subplots()
sns.lineplot(data = Apple_DS,
             x=Apple_DS.index,
             y='Open',
             color='blue', ax=ax,
             label = "Open Prices").set(xlabel = "Day",  title = "Apple Open Prices, Means, StdDevs")

sns.lineplot(data=rolling_mean, color = 'red', label = 'rolling means')
sns.lineplot(data=rolling_std, color = 'green', label = 'rolling stddev')

plt.show()

from statsmodels.tsa.stattools import adfuller

#chose AIC as the autolag paramater because we choose lag to reduce info criterion
adft = adfuller(Apple_DS["Open"], autolag = "AIC")

#retrieve the results
output_df = pd.DataFrame({"Values":[adft[0],adft[1],adft[2],adft[3], adft[4]['1%'], adft[4]['5%'], adft[4]['10%']]  , "Metric":["Test Statistics","p-value","No. of lags used","Number of observations used",
                                                        "critical value (1%)", "critical value (5%)", "critical value (10%)"]})
print(output_df)

#because the p-value (=0.223) is greater than 0.05, our data is not stationary.

#next we have to check for autocorrelation. This is a measure of how correlated
#time series data of step i is related to steps (1...i-1).
autocorrelation_lag1 = Apple_DS['Open'].autocorr(lag=1)
print("One month lag: ", autocorrelation_lag1)

#testing at 3, 6, 9 months
autocorrelation_lag3 = Apple_DS['Open'].autocorr(lag=3)
print("Three Month Lag: ", autocorrelation_lag3)

autocorrelation_lag6 = Apple_DS['Open'].autocorr(lag=6)
print("Six Month Lag: ", autocorrelation_lag6)

autocorrelation_lag9 = Apple_DS['Open'].autocorr(lag=9)
print("Nine Month Lag: ", autocorrelation_lag9)

#Next test is trend decomp, which is used to visualize relevant trends in the data.
from statsmodels.tsa.seasonal import seasonal_decompose

decompose = seasonal_decompose(Apple_DS['Open'],model='additive', period=24)
decompose.plot()
plt.show()

#FRom the decomposition, there seems to be a very irregular trend in the data.

#finally, we implement the time forecasting to measure future values.

#start with a train-test split of he data. Because we want to test on recent values,
#we do not split the data w shuffle because we want to preserve time-order.
train = Apple_DS[Apple_DS['Date'] < pd.to_datetime("2023-12-09", format='%Y-%m-%d')]
train['train'] = train['Open']

test = Apple_DS[Apple_DS['Date'] >= pd.to_datetime("2023-12-09", format='%Y-%m-%d')]
test['test'] = test['Open']

fig, ax = plt.subplots()
sns.lineplot(data = train,
             x=train.index,
             y='train', ax=ax).set(xlabel = "Day", title = "Apple Open")
sns.lineplot(data = test,  x=test.index,  y='test', ax=ax)
plt.show()

train2 = pd.concat([train['Date'], train['train']], axis=1, keys=['Date', 'train'])

test2 = pd.concat([test['Date'], test['test']], axis=1, keys=['Date', 'test'])

# Commented out IPython magic to ensure Python compatibility.
# %pip install pmdarima

#train an arima (time series) model.
from pmdarima.arima import auto_arima
model = auto_arima(train2['train'], trace=True, error_action='ignore', suppress_warnings=True)
model.fit(train2['train'])
forecast = model.predict(n_periods=len(test2['test']))
forecast = pd.DataFrame(forecast,index = test2.index,columns=['Prediction'])

fig, ax = plt.subplots()
sns.lineplot(data = train,
             x=train.index,
             y='train', ax=ax).set(xlabel = "Day", title = "Apple Open")
sns.lineplot(data = test,  x=test.index,  y='test', ax=ax)
sns.lineplot(data = forecast, x=forecast.index, y= 'Prediction', ax=ax, color= 'red')

plt.show()

from math import sqrt
from sklearn.metrics import mean_squared_error
rms = sqrt(mean_squared_error(test['Open'],forecast))
print("RMSE: ", rms)

#though there is a low error, it is obvious that the model did not perform that well based
#on the predictions that we observe in the chart.

# this didn't work too well with our data, possibly because there is limited data.
#so, we will try to use prophet, which is a Meta model used for data that is
#not stationary

from prophet import Prophet
import numpy as np

Apple_prophet_train = pd.concat([train['Date'], train['train']], axis=1, keys=['ds', 'y'])
Apple_prophet_test = pd.concat([test['Date'], test['test']], axis=1, keys=['ds', 'y'])

model = Prophet()
model.fit(Apple_prophet_train)

#predictions
open_prices = model.make_future_dataframe(periods=len(test))
forecast = model.predict(open_prices)

#rmse
forecast_valid = forecast['yhat'][487:]
rms=np.sqrt(np.mean(np.power((np.array(Apple_prophet_test['y'])-np.array(forecast_valid)),2)))
rms

#RMSE is much better than before.

Apple_prophet_test['Predictions'] = 0
Apple_prophet_test['Predictions'] = forecast_valid.values

fig, ax = plt.subplots(figsize=(15,6))
plt.plot(Apple_prophet_train['y'])
plt.plot(Apple_prophet_test[['y', 'Predictions']])
plt.show()

#based on the RMSE and the plot, it looks like this version of the time series analysis
#performs better than the Amira one.

#now straying away from time series analysis, we are trying a deep learning method

#more specifically, LSTM.

#importing required libraries
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM

#creating dataframe
data = Apple_DS
new_data = pd.DataFrame(index=range(0,len(data)),columns=['Date', 'Open'])
for i in range(0,len(data)):
    new_data['Date'][i] = data['Date'][i]
    new_data['Open'][i] = data['Open'][i]

#setting index
new_data.index = new_data.Date
new_data.drop('Date', axis=1, inplace=True)

#creating train and test sets
dataset = new_data.values

train = dataset[0:487,:]
valid = dataset[487:,:]

#converting dataset into x_train and y_train
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(dataset)

x_train, y_train = [], []
for i in range(60,len(train)):
    x_train.append(scaled_data[i-60:i,0])
    y_train.append(scaled_data[i,0])
x_train, y_train = np.array(x_train), np.array(y_train)

x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))

# create and fit the LSTM network
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],1)))
model.add(LSTM(units=50))
model.add(Dense(1))

model.compile(loss='mean_squared_error', optimizer='adam')
res = model.fit(x_train, y_train, epochs=5, batch_size=1, verbose=2)

#predicting 14 values, using past 60 from the train data
inputs = new_data[len(new_data) - len(valid) - 60:].values
inputs = inputs.reshape(-1,1)
inputs  = scaler.transform(inputs)

X_test = []
for i in range(60,inputs.shape[0]):
    X_test.append(inputs[i-60:i,0])
X_test = np.array(X_test)

X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))
open_price = model.predict(X_test)
open_price = scaler.inverse_transform(open_price)

history = res
plt.figure(figsize=(8,4))
plt.plot(history.history['loss'], label='Train Loss')
plt.title('LSTM loss')
plt.ylabel('loss')
plt.xlabel('epochs')
plt.legend(loc='upper right')
plt.show()

rms=np.sqrt(np.mean(np.power((valid-open_price),2)))
rms

#for plotting
fig, ax = plt.subplots(figsize=(15,6))
train = new_data[:487]
valid = new_data[487:]
valid['Predictions'] = open_price
plt.plot(train['Open'])
plt.plot(valid[['Open','Predictions']])
plt.show()

def generate_results_dataset(preds, ci):
    df = pd.DataFrame()
    df['prediction'] = preds
    if ci >= 0:
        df['upper'] = preds + ci
        df['lower'] = preds - ci
    else:
        df['upper'] = preds - ci
        df['lower'] = preds + ci

    return df

res = valid['Predictions']-valid['Open']
alpha=0.05
ci = np.quantile(res, 1 - alpha)

df = generate_results_dataset(valid['Predictions'], ci)
df.head()

rms = sqrt(mean_squared_error(valid['Open'],valid['Predictions']))
print("RMSE: ", rms)

#for plotting
fig, ax = plt.subplots(figsize=(15,6))
train = new_data[450:487]
valid = new_data[487:]
valid['Predictions'] = open_price
sns.despine(top=True)
sns.lineplot(x=valid.index, y= valid['Predictions'], label='predicted price')
ax.fill_between(x=df.index, y1 = df['lower'], y2 = df['upper'], alpha = 0.5, label ='95% CI')
sns.lineplot(valid['Open'], label='true price')
plt.title('2024 Predicted Open Prices')
plt.ylabel('Close Price ($)', size=15)
plt.xlabel('Date', size=15)
plt.legend(fontsize=15)
plt.show()

#going to do same as above but to predict the first two weeks of 2024.

Apple_2024 = pd.read_excel('2024_apple.xlsx',
                         index_col = None,
                         sheet_name = "Sheet1")

Apple_2024 = Apple_2024.reset_index()
date_vals = Apple_2024.Date.values
Apple_2024['Date'] = date_vals[::-1]

Apple_2024.head()

df2024 = Apple_2024.drop(['Open', 'High', 'Low', 'Volume'], axis=1)
df2022_23 = Apple_DS.drop(['Open', 'High', 'Low', 'Volume'], axis=1)

df2022_23.shape

df2022_23.head()

df2024.index = range(501, 510, 1)
df2024.index

df_all = pd.concat([df2022_23, df2024])

df_all.tail()

#importing required libraries
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM

#creating dataframe
data = df_all
new_data = pd.DataFrame(index=range(0,len(data)),columns=['Date', 'Close/Last'])
for i in range(0,len(data)):
    new_data['Date'][i] = data['Date'][i]
    new_data['Close/Last'][i] = data['Close/Last'][i]

#setting index
new_data.index = new_data.Date
new_data.drop('Date', axis=1, inplace=True)

#creating train and test sets
dataset = new_data.values

train = dataset[0:501,:]
valid = dataset[501:,:]

#converting dataset into x_train and y_train
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(dataset)

x_train, y_train = [], []
for i in range(60,len(train)):
    x_train.append(scaled_data[i-60:i,0])
    y_train.append(scaled_data[i,0])
x_train, y_train = np.array(x_train), np.array(y_train)

x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))

# create and fit the LSTM network
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],1)))
model.add(LSTM(units=50))
model.add(Dense(1))

model.compile(loss='mean_squared_error', optimizer='adam')
res = model.fit(x_train, y_train, epochs=7, batch_size=1, verbose=2)

#predicting 246 values, using past 60 from the train data
inputs = new_data[len(new_data) - len(valid) - 60:].values
inputs = inputs.reshape(-1,1)
inputs  = scaler.transform(inputs)

X_test = []
for i in range(60,inputs.shape[0]):
    X_test.append(inputs[i-60:i,0])
X_test = np.array(X_test)

X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))
close_price = model.predict(X_test)
close_price = scaler.inverse_transform(close_price)

#for plotting
fig, ax = plt.subplots(figsize=(15,6))
train = new_data[:501]
valid = new_data[501:]
valid['Predictions'] = close_price
plt.plot(train['Close/Last'])
plt.plot(valid[['Close/Last','Predictions']])
plt.show()

history = res
plt.figure(figsize=(8,4))
plt.plot(history.history['loss'], label='Train Loss')
plt.title('LSTM loss')
plt.ylabel('loss')
plt.xlabel('epochs')
plt.legend(loc='upper right')
plt.show()

res = valid['Predictions']-valid['Close/Last']
alpha=0.05
ci = np.quantile(res, 1 - alpha)

df = generate_results_dataset(valid['Predictions'], ci)
df.head()

rms = sqrt(mean_squared_error(valid['Close/Last'],valid['Predictions']))
print("RMSE: ", rms)

#for plotting
fig, ax = plt.subplots(figsize=(15,6))
train = new_data[450:501]
valid = new_data[501:]
valid['Predictions'] = close_price


sns.lineplot(x=valid.index, y= valid['Predictions'], label='predicted price')
ax.fill_between(x=df.index, y1 = df['lower'], y2 = df['upper'], alpha = 0.5, label ='95% CI')
sns.lineplot(x=valid.index, y= valid['Close/Last'], label = 'close price')

sns.despine(top=True)
plt.title('Predicted Close Prices, First 2 Weeks of 2024', fontsize = 15)
plt.ylabel('Close Price ($)', fontsize = 13)
plt.xlabel('Date', fontsize = 13)
plt.legend(loc = 'upper left', fontsize = 13)
plt.show()









